# -*- coding: utf-8 -*-
"""anomaly_detection_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10b4cT7V4s4l6s-MCOkX-6liZVXI9A9uo

# KDD Cup 99 Intrusion Detection
"""

# Installing Packages
!pip install kagglehub pandas numpy matplotlib seaborn scikit-learn

# Importing required library for path handling
import os

# Importing kagglehub for downloading dataset
import kagglehub

# Importing required libraries
import pandas as pd
import numpy as np

# Importing visualisation libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Importing sklearn utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Importing ML models
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, IsolationForest

# Downloading latest version of dataset
path = kagglehub.dataset_download("galaxyh/kdd-cup-1999-data")

# Printing dataset folder path
print("Path to dataset files:", path)

# Listing all downloaded files
files_list = os.listdir(path)
print("Files inside dataset folder:\n", files_list)

# Selecting dataset file
target_file_labeled = "kddcup.data_10_percent.gz"
DATA_PATH = os.path.join(path, target_file_labeled)
print("Using labeled dataset file:", DATA_PATH)

# Defining dataset configuration
KDD_COLUMNS = [
    "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes", "land",
    "wrong_fragment", "urgent", "hot", "num_failed_logins", "logged_in", "num_compromised",
    "root_shell", "su_attempted", "num_root", "num_file_creations", "num_shells",
    "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",
    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate",
    "srv_rerror_rate", "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
    "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",
    "dst_host_diff_srv_rate", "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate",
    "dst_host_serror_rate", "dst_host_srv_serror_rate", "dst_host_rerror_rate",
    "dst_host_srv_rerror_rate", "label"]

# Defining helper functions
def loading_kdd_data(file_path: str, columns: list) -> pd.DataFrame:
    if file_path.endswith(".gz"):
        df = pd.read_csv(file_path, names=columns, compression="gzip")
    else:
        df = pd.read_csv(file_path, names=columns)
    return df

def printing_basic_overview(df: pd.DataFrame) -> None:
    # Printing dataset shape
    print("\nDataset Shape:", df.shape)

    # Printing dataset info
    print("\nDataset Info:")
    print(df.info())

    # Printing first few rows
    print("\nDataset Preview (Head):")
    print(df.head())

    # Printing summary stats for numeric columns
    print("\nNumerical Summary:")
    print(df.describe())

    # Printing missing values
    print("\nMissing Values Count:")
    print(df.isnull().sum())


def plotting_label_distribution(df: pd.DataFrame, target_col: str) -> None:
    # Plotting label distribution
    plt.figure(figsize=(12, 5))
    sns.countplot(data=df, x=target_col)
    plt.title("Label Frequency Distribution")
    plt.xticks(rotation=90)
    plt.xlabel("Attack Label")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()


def plotting_categorical_distributions(df: pd.DataFrame, categorical_cols: list) -> None:
    # Plotting distributions for each categorical column
    for col in categorical_cols:
        plt.figure(figsize=(10, 4))
        sns.countplot(data=df, x=col)
        plt.title(f"Category Distribution: {col}")
        plt.xticks(rotation=90)
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.tight_layout()
        plt.show()


def plotting_protocol_traffic(df: pd.DataFrame) -> None:
    # Creating protocol-based traffic summary
    traffic_df = df.groupby("protocol_type")["label"].count().reset_index(name="counting")
    print("\nProtocol Traffic Summary:\n", traffic_df)

    # Plotting bar chart for protocol traffic
    plt.figure(figsize=(7, 4))
    sns.barplot(data=traffic_df, x="protocol_type", y="counting")
    plt.title("Connections Count by Protocol Type")
    plt.xlabel("Protocol")
    plt.ylabel("Connections")
    plt.tight_layout()
    plt.show()


def plotting_correlation_heatmap(df: pd.DataFrame) -> None:
    # Computing correlation matrix for numeric columns only
    corr = df.select_dtypes(include=[np.number]).corr()

    # Plotting correlation heatmap
    plt.figure(figsize=(16, 10))
    sns.heatmap(corr, cmap="coolwarm", linewidths=0.3)
    plt.title("Correlation Heatmap (Numeric Features)")
    plt.tight_layout()
    plt.show()


def encoding_categorical_features(df: pd.DataFrame, columns_to_encode: list) -> pd.DataFrame:
    # Copying dataset to avoid modifying original reference
    encoded_df = df.copy()

    # Initialising label encoder
    encoder = LabelEncoder()

    # Encoding categorical columns one by one
    for col in columns_to_encode:
        encoded_df[col] = encoder.fit_transform(encoded_df[col])

    return encoded_df


def splitting_data(df: pd.DataFrame, target_col: str, test_size: float = 0.30, seed: int = 42):
    # Separating features and target variable
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=seed, stratify=y
    )

    # Printing split sizes
    print("\nTrain/Test Split Details:")
    print("X_train shape:", X_train.shape)
    print("X_test shape:", X_test.shape)
    print("y_train shape:", y_train.shape)
    print("y_test shape:", y_test.shape)

    return X_train, X_test, y_train, y_test


def evaluating_classifier(model_name: str, y_true, y_pred) -> float:
    # Calculating accuracy
    acc = accuracy_score(y_true, y_pred) * 100

    # Printing model results
    print(f"\nModel Evaluation: {model_name}")
    print(f"Accuracy: {acc:.2f}%")
    print("\nClassification Report:\n", classification_report(y_true, y_pred, zero_division=0))

    return acc


def plotting_confusion(y_true, y_pred, title: str) -> None:
    # Computing confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plotting confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()


def plotting_model_comparison(names: list, scores: list) -> None:
    # Plotting model accuracy comparison
    plt.figure(figsize=(7, 4))
    plt.bar(names, scores)
    plt.title("Classifier Accuracy Comparison")
    plt.xlabel("Model")
    plt.ylabel("Accuracy (%)")
    plt.ylim(0, 100)
    plt.tight_layout()
    plt.show()


def running_isolation_forest(X_train: pd.DataFrame, X_test: pd.DataFrame) -> pd.DataFrame:
    # Initialising isolation forest model
    anomaly_model = IsolationForest(random_state=42, contamination="auto")

    # Fitting model on training set
    anomaly_model.fit(X_train)

    # Predicting anomalies on test set (-1 means anomaly)
    anomaly_flags = anomaly_model.predict(X_test)

    # Extracting anomalous samples
    anomaly_df = X_test[anomaly_flags == -1].copy()

    # Printing anomalies count
    print("\nAnomaly Detection Results:")
    print("Total anomalies detected:", anomaly_df.shape[0])
    return anomaly_df

def plotting_feature_importance(model, feature_names: list, title: str) -> None:
    # Checking if model supports feature importance
    if not hasattr(model, "feature_importances_"):
        print(f"\n{title}: Feature importance not available for this model.")
        return

    # Creating importance dataframe
    imp_df = pd.DataFrame({
        "Feature": feature_names,
        "Importance": model.feature_importances_
    }).sort_values("Importance", ascending=False)

    # Plotting top important features
    plt.figure(figsize=(10, 7))
    sns.barplot(data=imp_df.head(15), x="Importance", y="Feature")
    plt.title(title)
    plt.xlabel("Importance Score")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()

# Loading KDD dataset using KaggleHub path
data = loading_kdd_data(DATA_PATH, KDD_COLUMNS)

# Printing dataset overview
printing_basic_overview(data)

# Printing label distribution counts
print("\nLabel Value Counts:\n", data["label"].value_counts())

# Plotting label distribution
plotting_label_distribution(data, "label")

# Plotting categorical distributions
categorical_cols = ["protocol_type", "service", "flag"]
plotting_categorical_distributions(data, categorical_cols)

# Plotting protocol traffic analysis
plotting_protocol_traffic(data)

# Plotting correlation heatmap
plotting_correlation_heatmap(data)

# Encoding categorical columns including label
data_encoded = encoding_categorical_features(data, ["protocol_type", "service", "flag", "label"])

# Splitting dataset into train/test
X_train, X_test, y_train, y_test = splitting_data(data_encoded, target_col="label", test_size=0.30, seed=42)

# Training Random Forest model
rf_model = RandomForestClassifier(n_estimators=150, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

# Evaluating Random Forest model
rf_acc = evaluating_classifier("Random Forest", y_test, rf_pred)
plotting_confusion(y_test, rf_pred, "Confusion Matrix of Random Forest")

# Training Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)

# Evaluating Decision Tree model
dt_acc = evaluating_classifier("Decision Tree", y_test, dt_pred)
plotting_confusion(y_test, dt_pred, "Confusion Matrix of Decision Tree")

# Comparing model accuracy
plotting_model_comparison(["Random Forest", "Decision Tree"], [rf_acc, dt_acc])

# Running anomaly detection
anomaly_data = running_isolation_forest(X_train, X_test)

# Plotting feature importances
plotting_feature_importance(rf_model, X_train.columns.tolist(), "Top Feature Importances of Random Forest")
plotting_feature_importance(dt_model, X_train.columns.tolist(), "Top Feature Importances of Decision Tree")